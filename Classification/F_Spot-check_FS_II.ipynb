{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spot-check EEG classification models based on all features\n",
    "In this script we spot-check different EEG classification models to clasify different epochs of one subject into resting state, left hand movement and right hand movement using leave one subject out cross-validation, performing a correlation based features selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict of some models to evaluate them {key:model name} {values:model object}\n",
    "def define_models(models=dict()) :\n",
    "    models['logistic'] = LogisticRegression()\n",
    "    models['lda'] = LinearDiscriminantAnalysis()\n",
    "    models['qda'] = QuadraticDiscriminantAnalysis()\n",
    "    models['svcl'] = SVC(kernel='linear',probability=True)\n",
    "    models['svmp'] = SVC(kernel='poly',probability=True)\n",
    "    # non-linear models\n",
    "    models['cart'] = DecisionTreeClassifier()\n",
    "    models['knn'] = KNeighborsClassifier()  \n",
    "    # ensemble models\n",
    "    models['ada'] = AdaBoostClassifier()\n",
    "    models['rf'] = RandomForestClassifier()\n",
    "    models['mlp'] = MLPClassifier()\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a single model\n",
    "def evaluate_model(X, y, model, subjects_train):\n",
    "    # evaluate model\n",
    "    scores = dict()\n",
    "    scores['accuracy'] = []\n",
    "    scores['roc_auc'] = []\n",
    "    scores['f1'] = []\n",
    "    for subject in subjects_train :\n",
    "        train_idx = subjects_idx_train != subject\n",
    "        X_training = X[train_idx.values]\n",
    "        y_training = y[train_idx.values]\n",
    "        val_idx = subjects_idx_train == subject\n",
    "        X_val = X[val_idx.values]\n",
    "        y_val = y[val_idx.values]\n",
    "        #train the model\n",
    "        model.fit(X_training,y_training)\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_prob = model.predict_proba(X_val)\n",
    "        scores['accuracy'].append(accuracy_score(y_val,y_pred))\n",
    "        scores['roc_auc'].append(roc_auc_score(y_val,y_prob,multi_class='ovo',labels=[0,1,2]))\n",
    "        scores['f1'].append(f1_score(y_val,y_pred,average='weighted'))\n",
    "    print(scores)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a dict of models {name:object}, returns {name:score}\n",
    "def evaluate_models(X, y, models, subjects_train):\n",
    "    results = dict()\n",
    "    metric='accuracy'\n",
    "    for name, model in models.items():\n",
    "        scores = evaluate_model(X, y, model, subjects_train)\n",
    "        results[name] = scores\n",
    "        print(name,'evaluated')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print and plot the results of the models in order (from the best one to the worse one)\n",
    "def summarize_results(results):\n",
    "    # create a list of (name, mean(scores)) tuples\n",
    "    mean_accuracy_scores = dict()\n",
    "    mean_auc_scores = dict()\n",
    "    mean_f1_scores = dict()\n",
    "    for k,v in results.items() :\n",
    "        mean_accuracy_scores.update({k : mean(v['accuracy'])})\n",
    "        mean_auc_scores.update({k : mean(v['roc_auc'])})\n",
    "        mean_f1_scores.update({k : mean(v['f1'])})\n",
    "    # sort tuples by mean score\n",
    "    ordered_accuracy = sorted(mean_accuracy_scores.items(), key=lambda x: x[1])\n",
    "    # reverse for descending order (from the best one to the worse one)\n",
    "    ordered_accuracy = list(reversed(ordered_accuracy))\n",
    "    # retrieve the top for summarization\n",
    "    names = [x[0] for x in ordered_accuracy]\n",
    "    accuracy_scores = [results[x]['accuracy'] for x in names]\n",
    "    auc_scores = [results[x]['roc_auc'] for x in names]\n",
    "    f1_scores = [results[x]['f1'] for x in names]\n",
    "    # print the top \n",
    "    print()\n",
    "    for i in range(len(results)):\n",
    "        name = names[i]\n",
    "        mean_accuracy_score = mean_accuracy_scores[name]\n",
    "        mean_auc_score = mean_auc_scores[name]\n",
    "        mean_f1 = mean_f1_scores[name]\n",
    "        print('Rank=%d, Name=%s, Accuracy=%.3f, roc_auc=%.3f, f1=%.3f' % (i+1, name, mean_accuracy_score, mean_auc_score, mean_f1))\n",
    "    # boxplot for the top n\n",
    "    print(mean_accuracy_score, names)\n",
    "    plt.figure()\n",
    "    plt.boxplot(accuracy_scores, labels=names)\n",
    "    _, labels = pyplot.xticks()\n",
    "    pyplot.setp(labels, rotation=90)\n",
    "    plt.title('Accuracy scores')\n",
    "    plt.figure()\n",
    "    plt.boxplot(auc_scores, labels=names)\n",
    "    _, labels = pyplot.xticks()\n",
    "    pyplot.setp(labels, rotation=90)\n",
    "    plt.title('ROC-AUC scores')\n",
    "    plt.figure()\n",
    "    plt.boxplot(f1_scores, labels=names)\n",
    "    _, labels = pyplot.xticks()\n",
    "    pyplot.setp(labels, rotation=90)\n",
    "    plt.title('F1 scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read features and labels from all subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read subjects files and merge them\n",
    "files = os.listdir('Subjects data')\n",
    "features_all = pd.read_csv('Subjects data/' + files[0], encoding='latin1')\n",
    "files.pop(0)\n",
    "for f in files :\n",
    "    features_subject = pd.read_csv('Subjects data/' + f, encoding='latin1')\n",
    "    frames = [features_all, features_subject]\n",
    "    features_all = pd.concat(frames)\n",
    "#features_all.index = [features_all['Subject'], features_all['Epoch']]\n",
    "subjects_idx = features_all['Subject'].values\n",
    "#features_all = features_all.drop(['Subject','Epoch'], axis = 1)\n",
    "features_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select randomly subjects for train and test\n",
    "subjects = np.unique(subjects_idx)\n",
    "subjects_train = [15, 12, 21, 4, 10, 44, 3, 18, 29, 20, 7, 19, 11, 2, 28, 5, 8, 16, 6]\n",
    "subjects_test = [1, 13, 17, 14, 9]\n",
    "subjects_train, subjects_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = features_all['Subject'] == subjects_train[0]\n",
    "features_train = features_all[train_idx]\n",
    "subjects_train2 = np.delete(subjects_train, 0, axis=None)\n",
    "for subject in subjects_train2 :\n",
    "    train_idx = features_all['Subject'] == subject\n",
    "    frames = [features_train, features_all[train_idx]]\n",
    "    features_train = pd.concat(frames)\n",
    "subjects_idx_train = features_train['Subject']\n",
    "features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train.index = [features_train['Subject'], features_train['Epoch']]\n",
    "X_train = features_train.drop(['Subject','Epoch','Label'], axis = 1)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = features_train['Label']\n",
    "Counter(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the most highly correlated features with the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the features that have a correlation higher than 0.5 with the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Pearson Correlation\n",
    "f = plt.figure(figsize=(12,10))\n",
    "features = features_train.drop(['Subject','Epoch'], axis=1)\n",
    "cor = features.corr()\n",
    "plt.matshow(cor, fignum=f.number)\n",
    "plt.xticks(range(features.shape[1]), features.columns, fontsize=14, rotation=45)\n",
    "plt.yticks(range(features.shape[1]), features.columns, fontsize=14)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "plt.title('Correlation Matrix', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation with output variable\n",
    "cor_target = abs(cor['Label'])\n",
    "#Selecting highly correlated features\n",
    "relevant_features = cor_target[cor_target > 0.2]\n",
    "selected_features = relevant_features[0:len(relevant_features)-1].index\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[selected_features]\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spot-check algorithms standardizing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the features matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[selected_features]\n",
    "standardizer = StandardScaler()\n",
    "X_scaled_train = standardizer.fit_transform(X_train)\n",
    "X_scaled_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model list\n",
    "models = define_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate models\n",
    "results = evaluate_models(X_scaled_train, Y_train, models, subjects_train)\n",
    "# summarize results\n",
    "summarize_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing the data we obtain that the best three models are Logistic, LDA and SCVL with mean estimated accuracies of 0.509, 0.504 and 0.494 respectively. Let's try to normalize the standardized dataset to see if we can improve the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spot-check algorithms standardizing and normalizing data\n",
    "Preprocess the training features matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardizer = StandardScaler()\n",
    "X_scaled_train = standardizer.fit_transform(X_train)\n",
    "normalizer = MinMaxScaler(feature_range=(0,1))\n",
    "X_scaled_train = normalizer.fit_transform(X_scaled_train)\n",
    "X_scaled_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate models\n",
    "results = evaluate_models(X_scaled_train, Y_train, models, subjects_train)\n",
    "# summarize results\n",
    "summarize_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete features that are highly correlated with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Pearson Correlation\n",
    "f = plt.figure(figsize=(12,10))\n",
    "cor = X_train.corr()\n",
    "plt.matshow(cor, fignum=f.number)\n",
    "plt.xticks(range(X_train.shape[1]), X_train.columns, fontsize=14, rotation=45)\n",
    "plt.yticks(range(X_train.shape[1]), X_train.columns, fontsize=14)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "plt.title('Correlation Matrix', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_features = []\n",
    "for i in range(len(cor.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(cor.iloc[i, j]) > 0.9:\n",
    "            colname = cor.columns[i]\n",
    "            correlated_features.append(colname)\n",
    "correlated_features = list(dict.fromkeys(correlated_features))\n",
    "correlated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(correlated_features, axis=1)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Pearson Correlation\n",
    "f = plt.figure(figsize=(12,10))\n",
    "cor = X_train.corr()\n",
    "plt.matshow(cor, fignum=f.number)\n",
    "plt.xticks(range(X_train.shape[1]), X_train.columns, fontsize=14, rotation=45)\n",
    "plt.yticks(range(X_train.shape[1]), X_train.columns, fontsize=14)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "plt.title('Correlation Matrix', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we obtain that performing dimensionality reduction with PCA does not improve the model performance. Maybe with other dimensionality reduction/feature selection techniques? From the moment, let's try to perform PCA, standardize and normalize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spot-check standardizing the data\n",
    "Preprocess the training features matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardizer = StandardScaler()\n",
    "X_scaled_train = standardizer.fit_transform(X_train)\n",
    "X_scaled_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate models\n",
    "results = evaluate_models(X_scaled_train, Y_train, models, subjects_train)\n",
    "# summarize results\n",
    "summarize_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spot-check standardizing and normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardizer = StandardScaler()\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "X_scaled_train = standardizer.fit_transform(X_train)\n",
    "X_scaled_train = scaler.fit_transform(X_scaled_train)\n",
    "X_scaled_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate models\n",
    "results = evaluate_models(X_scaled_train, Y_train, models, subjects_train)\n",
    "# summarize results\n",
    "summarize_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
