{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spot-check EEG classification models based on all features\n",
    "In this script we spot-check different EEG classification models to clasify different epochs of one subject into resting state, left hand movement and right hand movement using leave one subject out cross-validation, selecting the features with recursive feature elimination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict of some models to evaluate them {key:model name} {values:model object}\n",
    "def define_models(models=dict()) :\n",
    "    models['logistic'] = LogisticRegression()\n",
    "    models['lda'] = LinearDiscriminantAnalysis()\n",
    "    models['qda'] = QuadraticDiscriminantAnalysis()\n",
    "    models['svcl'] = SVC(kernel='linear',probability=True)\n",
    "    models['svmp'] = SVC(kernel='poly',probability=True)\n",
    "    # non-linear models\n",
    "    models['cart'] = DecisionTreeClassifier()\n",
    "    models['knn'] = KNeighborsClassifier()  \n",
    "    # ensemble models\n",
    "    models['ada'] = AdaBoostClassifier()\n",
    "    models['rf'] = RandomForestClassifier()\n",
    "    models['mlp'] = MLPClassifier()\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a single model\n",
    "def evaluate_model(X, y, model, subjects_train):\n",
    "    # evaluate model\n",
    "    scores = dict()\n",
    "    scores['accuracy'] = []\n",
    "    scores['roc_auc'] = []\n",
    "    scores['f1'] = []\n",
    "    for subject in subjects_train :\n",
    "        train_idx = subjects_idx_train != subject\n",
    "        X_training = X[train_idx.values]\n",
    "        y_training = y[train_idx.values]\n",
    "        val_idx = subjects_idx_train == subject\n",
    "        X_val = X[val_idx.values]\n",
    "        y_val = y[val_idx.values]\n",
    "        #train the model\n",
    "        model.fit(X_training,y_training)\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_prob = model.predict_proba(X_val)\n",
    "        scores['accuracy'].append(accuracy_score(y_val,y_pred))\n",
    "        scores['roc_auc'].append(roc_auc_score(y_val,y_prob,multi_class='ovo',labels=[0,1,2]))\n",
    "        scores['f1'].append(f1_score(y_val,y_pred,average='weighted'))\n",
    "    print(scores)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a dict of models {name:object}, returns {name:score}\n",
    "def evaluate_models(X, y, models, subjects_train):\n",
    "    results = dict()\n",
    "    metric='accuracy'\n",
    "    for name, model in models.items():\n",
    "        scores = evaluate_model(X, y, model, subjects_train)\n",
    "        results[name] = scores\n",
    "        print(name,'evaluated')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print and plot the results of the models in order (from the best one to the worse one)\n",
    "def summarize_results(results):\n",
    "    # create a list of (name, mean(scores)) tuples\n",
    "    mean_accuracy_scores = dict()\n",
    "    mean_auc_scores = dict()\n",
    "    mean_f1_scores = dict()\n",
    "    for k,v in results.items() :\n",
    "        mean_accuracy_scores.update({k : mean(v['accuracy'])})\n",
    "        mean_auc_scores.update({k : mean(v['roc_auc'])})\n",
    "        mean_f1_scores.update({k : mean(v['f1'])})\n",
    "    # sort tuples by mean score\n",
    "    ordered_accuracy = sorted(mean_accuracy_scores.items(), key=lambda x: x[1])\n",
    "    # reverse for descending order (from the best one to the worse one)\n",
    "    ordered_accuracy = list(reversed(ordered_accuracy))\n",
    "    # retrieve the top for summarization\n",
    "    names = [x[0] for x in ordered_accuracy]\n",
    "    accuracy_scores = [results[x]['accuracy'] for x in names]\n",
    "    auc_scores = [results[x]['roc_auc'] for x in names]\n",
    "    f1_scores = [results[x]['f1'] for x in names]\n",
    "    # print the top \n",
    "    print()\n",
    "    for i in range(len(results)):\n",
    "        name = names[i]\n",
    "        mean_accuracy_score = mean_accuracy_scores[name]\n",
    "        mean_auc_score = mean_auc_scores[name]\n",
    "        mean_f1 = mean_f1_scores[name]\n",
    "        print('Rank=%d, Name=%s, Accuracy=%.3f, roc_auc=%.3f, f1=%.3f' % (i+1, name, mean_accuracy_score, mean_auc_score, mean_f1))\n",
    "    # boxplot for the top n\n",
    "    print(mean_accuracy_score, names)\n",
    "    plt.figure()\n",
    "    plt.boxplot(accuracy_scores, labels=names)\n",
    "    _, labels = pyplot.xticks()\n",
    "    pyplot.setp(labels, rotation=90)\n",
    "    plt.title('Accuracy scores')\n",
    "    plt.figure()\n",
    "    plt.boxplot(auc_scores, labels=names)\n",
    "    _, labels = pyplot.xticks()\n",
    "    pyplot.setp(labels, rotation=90)\n",
    "    plt.title('ROC-AUC scores')\n",
    "    plt.figure()\n",
    "    plt.boxplot(f1_scores, labels=names)\n",
    "    _, labels = pyplot.xticks()\n",
    "    pyplot.setp(labels, rotation=90)\n",
    "    plt.title('F1 scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read features and labels from all subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read subjects files and merge them\n",
    "files = os.listdir('Subjects data')\n",
    "features_all = pd.read_csv('Subjects data/' + files[0], encoding='latin1')\n",
    "files.pop(0)\n",
    "for f in files :\n",
    "    features_subject = pd.read_csv('Subjects data/' + f, encoding='latin1')\n",
    "    frames = [features_all, features_subject]\n",
    "    features_all = pd.concat(frames)\n",
    "#features_all.index = [features_all['Subject'], features_all['Epoch']]\n",
    "subjects_idx = features_all['Subject'].values\n",
    "#features_all = features_all.drop(['Subject','Epoch'], axis = 1)\n",
    "features_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select randomly subjects for train and test\n",
    "subjects = np.unique(subjects_idx)\n",
    "subjects_train = [15, 12, 21, 4, 10, 44, 3, 18, 29, 20, 7, 19, 11, 2, 28, 5, 8, 16, 6]\n",
    "subjects_test = [1, 13, 17, 14, 9]\n",
    "subjects_train, subjects_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = features_all['Subject'] == subjects_train[0]\n",
    "features_train = features_all[train_idx]\n",
    "subjects_train2 = np.delete(subjects_train, 0, axis=None)\n",
    "for subject in subjects_train2 :\n",
    "    train_idx = features_all['Subject'] == subject\n",
    "    frames = [features_train, features_all[train_idx]]\n",
    "    features_train = pd.concat(frames)\n",
    "subjects_idx_train = features_train['Subject']\n",
    "features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train.index = [features_train['Subject'], features_train['Epoch']]\n",
    "X_train = features_train.drop(['Subject','Epoch','Label'], axis = 1)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = features_train['Label']\n",
    "Counter(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select features with recursive features elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature extraction\n",
    "#model = LogisticRegression(max_iter=600)\n",
    "#rfe = RFE(model,20)\n",
    "#fit = rfe.fit(X_train.values, Y_train.values)\n",
    "#indexes = []\n",
    "#for i, x in enumerate(fit.support_) :\n",
    "#    if x :\n",
    "#        indexes.append(i)\n",
    "#X_train = X_train.iloc[:,indexes]\n",
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction\n",
    "selected_features = ['PSD_FC1_D3', 'PSD_Cz_D3', 'PSD_CP1_D3', 'PSD_CP2_D3', 'PLV_FC5_FC1',\n",
    "                        'PLV_FC5_CP5', 'PLV_FC3_C1', 'PLV_FC3_CP1', 'PLV_FC1_FC2', 'PLV_FC1_C3',\n",
    "                       'PLV_FC1_C2', 'PLV_FC1_CPz', 'PLV_C5_CP6', 'PLV_C3_C1', 'PLV_Cz_C4',\n",
    "                       'PLV_C2_C4', 'PLV_CP5_CP3', 'PLV_CP5_CP6', 'PLV_CP3_CP2', 'PLV_CP4_CP6']\n",
    "X_train = X_train[selected_features]\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spot-check algorithms standardizing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the features matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardizer = StandardScaler()\n",
    "X_scaled_train = standardizer.fit_transform(X_train)\n",
    "X_scaled_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model list\n",
    "models = define_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate models\n",
    "results = evaluate_models(X_scaled_train, Y_train.values, models, subjects_train)\n",
    "# summarize results\n",
    "summarize_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing the data we obtain that the best three models are Logistic, LDA and SCVL with mean estimated accuracies of 0.509, 0.504 and 0.494 respectively. Let's try to normalize the standardized dataset to see if we can improve the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spot-check algorithms standardizing and normalizing data\n",
    "Preprocess the training features matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardizer = StandardScaler()\n",
    "X_scaled_train = standardizer.fit_transform(X_train)\n",
    "normalizer = MinMaxScaler(feature_range=(0,1))\n",
    "X_scaled_train = normalizer.fit_transform(X_scaled_train)\n",
    "X_scaled_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate models\n",
    "results = evaluate_models(X_scaled_train, Y_train, models, subjects_train)\n",
    "# summarize results\n",
    "summarize_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we obtain that standardizing and normalizing the data the best three models are SVCL, SVMP and Logistic with mean estimated accuracies of 0.638, 0.614 and 0.602 respectively. Better results that only standardizing the data. Let's try to perform some dimensionality reduction (PCA) to see if we can improve the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous spot-checks we have obtained that the best pre-procesing technique would be to standardize and normalize the data and not to apply PCA. The 3 best models with this technique are MLP, SVMP and Logistic. Let's tune the parameters of each model to select the best ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
